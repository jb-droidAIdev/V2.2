# Calibration System Implementation - Complete Documentation

## üìã Overview

This document provides a comprehensive overview of the fully implemented Calibration System for the QMS application. The system supports Reproducibility, Repeatability, and Accuracy calibration workflows as defined in the QA Workflow document.

---

## üóÑÔ∏è Database Schema

### Models

#### 1. **CalibrationSession**
Main session entity that orchestrates the entire calibration workflow.

**Fields:**
- `id`: Unique identifier
- `campaignId`: Associated campaign
- `title`: Session title
- `description`: Optional description
- `scheduledAt`: When the session is scheduled
- `status`: Current status (SCHEDULED, ANCHOR_PENDING, SCORING_OPEN, SCORING_CLOSED, COMPLETED, CANCELLED)
- Configuration fields for ticket counts and score ranges
- Target thresholds (targetRnR, targetAccuracy)
- Calculated metrics (avgReproducibility, avgRepeatability, totalRange, calculatedRnR, avgAccuracyGap)
- Timestamps for workflow stages

**Relations:**
- `campaign`: Campaign
- `createdBy`: User who created the session
- `participants`: CalibrationParticipant[]
- `tickets`: CalibrationTicket[]
- `scores`: CalibrationScore[]
- `anchors`: CalibrationAnchor[]
- `results`: CalibrationResult[]

#### 2. **CalibrationParticipant**
Tracks users participating in a calibration session.

**Fields:**
- `id`: Unique identifier
- `sessionId`: Associated session
- `userId`: Participant user
- `role`: RATER, QA_TL, or AM_SDM
- `hasCompletedScoring`: Completion status
- `completedAt`: Completion timestamp

#### 3. **CalibrationTicket**
Individual tickets to be scored in a session.

**Fields:**
- `id`: Unique identifier
- `sessionId`: Associated session
- `auditId`: Reference to original audit
- `ticketId`: Original ticket ID
- `type`: REPRODUCIBILITY, REPEATABILITY, or ACCURACY
- `passNumber`: For repeatability (1 or 2)
- `groupId`: Links first and second pass
- `scoreRange`: HIGH, MID, or LOW (for accuracy)
- `anchorScore`: Validated correct score (for accuracy)
- Metadata fields

#### 4. **CalibrationScore**
Individual scores submitted by raters.

**Fields:**
- `id`: Unique identifier
- `sessionId`: Associated session
- `ticketId`: Ticket being scored
- `participantId`: Rater who scored
- `totalScore`: Score value
- `scoreDetails`: JSON breakdown by criterion
- `scoredAt`: Timestamp

**Unique Constraint:** Each rater scores each ticket once

#### 5. **CalibrationAnchor**
Anchor tickets for accuracy calibration requiring validation.

**Fields:**
- `id`: Unique identifier
- `sessionId`: Associated session
- `auditId`: Reference audit
- `scoreRange`: HIGH, MID, or LOW
- `score`: Anchor score
- `status`: PENDING_VALIDATION, VALIDATED, REJECTED, NON_MATCHING
- QA TL approval fields
- AM/SDM approval fields
- `rejectionReason`: If rejected

#### 6. **CalibrationResult**
Calculated results for sessions and individual raters.

**Fields:**
- `id`: Unique identifier
- `sessionId`: Associated session
- `userId`: Null for session-level, populated for individual results
- Metric fields (avgReproducibility, avgRepeatability, avgAccuracyGap, totalRange, calculatedRnR)
- Pass/fail indicators
- `calculatedAt`: Timestamp

---

## üîß Backend API

### Base URL: `/calibration`

### Session Management

#### `POST /calibration/sessions`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Body:** CreateCalibrationSessionDto  
**Returns:** Created session with participants

**Request Body:**
```json
{
  "campaignId": "uuid",
  "title": "Monthly QA Calibration - January 2026",
  "description": "Quarterly calibration session",
  "scheduledAt": "2026-01-15T10:00:00Z",
  "reproducibilityTicketCount": 4,
  "repeatabilityTicketCount": 2,
  "accuracyTicketCount": 6,
  "highScoreMin": 95,
  "highScoreMax": 100,
  "midScoreMin": 88,
  "midScoreMax": 94,
  "lowScoreMin": 0,
  "lowScoreMax": 87,
  "targetRnR": 15.0,
  "targetAccuracy": 5.0,
  "raterUserIds": ["user1", "user2", "user3"],
  "qaTlUserId": "qa-tl-user",
  "amSdmUserId": "am-user"
}
```

#### `GET /calibration/sessions`
**Permissions:** QA_TL, AM, SDM, ADMIN, QA  
**Query Params:** campaignId?, status?, page?, limit?  
**Returns:** Paginated list of sessions

#### `GET /calibration/sessions/:id`
**Permissions:** QA_TL, AM, SDM, ADMIN, QA  
**Returns:** Full session details with tickets, scores, anchors, results

#### `PUT /calibration/sessions/:id`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Body:** UpdateCalibrationSessionDto  
**Returns:** Updated session

#### `DELETE /calibration/sessions/:id`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** 204 No Content

### Randomization

#### `POST /calibration/sessions/:id/randomize`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** Success message  
**Action:** Randomizes all tickets (Reproducibility, Repeatability, Accuracy anchors) and updates status to ANCHOR_PENDING

### Anchor Validation

#### `POST /calibration/anchors/:id/validate`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Body:**
```json
{
  "anchorId": "uuid",
  "approved": true,
  "rejectionReason": "Optional reason if rejected"
}
```
**Returns:** Updated anchor  
**Action:** 
- Records QA TL or AM/SDM approval
- If both approve ‚Üí VALIDATED
- If either rejects ‚Üí NON_MATCHING, triggers replacement
- When all 6 anchors validated ‚Üí Opens scoring

#### `GET /calibration/sessions/:id/anchors`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** List of anchors for the session

### Scoring

#### `POST /calibration/scores`
**Permissions:** QA, QA_TL, ADMIN  
**Body:**
```json
{
  "sessionId": "uuid",
  "ticketId": "uuid",
  "totalScore": 95.5,
  "scoreDetails": {
    "criterion1": 10,
    "criterion2": 8.5
  }
}
```
**Returns:** Created/updated score  
**Action:**
- Upserts score
- Checks participant completion
- When all raters complete ‚Üí Closes scoring and auto-calculates results

#### `GET /calibration/sessions/:id/tickets`
**Permissions:** QA, QA_TL, ADMIN  
**Returns:** List of tickets for scoring

#### `GET /calibration/sessions/:id/scores`
**Permissions:** QA, QA_TL, AM, SDM, ADMIN  
**Returns:** All scores for the session

### Results & Calculations

#### `POST /calibration/sessions/:id/calculate`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** Calculated metrics  
**Action:** Manually trigger calculation (normally auto-triggered)

#### `GET /calibration/sessions/:id/results`
**Permissions:** QA, QA_TL, AM, SDM, ADMIN  
**Returns:**
```json
{
  "session": {
    "id": "uuid",
    "title": "Session Title",
    "status": "COMPLETED",
    "avgReproducibility": 2.5,
    "avgRepeatability": 1.8,
    "totalRange": 15.0,
    "calculatedRnR": 28.67,
    "avgAccuracyGap": 3.2,
    "targetRnR": 15.0,
    "targetAccuracy": 5.0
  },
  "results": [
    {
      "userId": "user1",
      "avgAccuracyGap": 2.8,
      "passedAccuracy": true
    }
  ]
}
```

### User-Specific Endpoints

#### `GET /calibration/my-tasks`
**Permissions:** QA, QA_TL, AM, SDM, ADMIN  
**Query Params:** status?  
**Returns:** Sessions where user is a participant

#### `GET /calibration/my-tasks/:sessionId`
**Permissions:** QA, QA_TL, AM, SDM, ADMIN  
**Returns:** Detailed task info with progress

### Statistics & Analytics

#### `GET /calibration/statistics/campaign/:campaignId`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** Campaign-level calibration statistics

#### `GET /calibration/statistics/overview`
**Permissions:** QA_TL, AM, SDM, ADMIN  
**Returns:** System-wide calibration overview

---

## üìä Calculation Formulas

### Reproducibility (Between-Rater Consistency)
For each ticket, calculate the standard deviation of all rater scores:
```
œÉ = ‚àö(Œ£(xi - Œº)¬≤ / n)
```
Average all standard deviations across tickets.

### Repeatability (Within-Rater Consistency)
For each rater, calculate the absolute difference between their two scores on the same ticket:
```
Œî = |Score_Pass1 - Score_Pass2|
```
Average deltas per rater, then average across all raters.

### Total Range
```
Range = Max(all scores) - Min(all scores)
```

### R&R Percentage
```
R&R% = ((Reproducibility + Repeatability) / Total Range) √ó 100
```
**Pass Threshold:** < 15% (configurable)

### Accuracy
For each rater on each accuracy ticket:
```
Gap = |Rater Score - Anchor Score|
```
Average all gaps.

**Pass Threshold:** < 5 points (configurable)

---

## üîÑ Workflow States

### State Transitions

```
SCHEDULED
    ‚Üì (Randomize tickets)
ANCHOR_PENDING
    ‚Üì (All anchors validated)
SCORING_OPEN
    ‚Üì (All raters complete)
SCORING_CLOSED
    ‚Üì (Auto-calculate)
COMPLETED
```

**CANCELLED:** Can be set manually at any time

---

## üéØ Key Features

### Automatic Workflows
- ‚úÖ Auto-replacement of rejected anchors
- ‚úÖ Auto-open scoring when anchors validated
- ‚úÖ Auto-close scoring when all raters complete
- ‚úÖ Auto-calculate results on scoring close

### Validation & Constraints
- ‚úÖ Each rater scores each ticket exactly once
- ‚úÖ Both QA TL and AM/SDM must approve anchors
- ‚úÖ Minimum ticket counts enforced
- ‚úÖ Score range validation

### Traceability
- ‚úÖ Full audit trail of approvals
- ‚úÖ Timestamps for all workflow stages
- ‚úÖ Individual and session-level results

---

## üöÄ Next Steps

### Frontend Implementation Required

1. **Session Management UI**
   - Create session form
   - Session list/dashboard
   - Session details view

2. **Anchor Validation UI**
   - Anchor review interface for QA TL
   - Anchor review interface for AM/SDM
   - Approval/rejection workflow

3. **Scoring UI**
   - Ticket scoring interface for raters
   - Progress tracking
   - Score submission

4. **Results Dashboard**
   - Session results visualization
   - Individual rater performance
   - Campaign-level analytics
   - Charts and graphs for R&R metrics

---

## üìù Testing Checklist

- [ ] Create calibration session
- [ ] Randomize tickets
- [ ] Validate anchors (both approve)
- [ ] Validate anchors (one rejects - verify replacement)
- [ ] Submit scores as rater
- [ ] Verify auto-close on completion
- [ ] Verify calculation accuracy
- [ ] Check individual results
- [ ] Test permission guards
- [ ] Test edge cases (insufficient audits, etc.)

---

## üéâ Implementation Status

‚úÖ **Database Schema** - Complete  
‚úÖ **DTOs** - Complete  
‚úÖ **Service Layer** - Complete  
‚úÖ **Controller** - Complete  
‚úÖ **Module Wiring** - Complete  
‚è≥ **Frontend UI** - Pending  
‚è≥ **Integration Testing** - Pending

**Backend API is fully functional and ready for frontend integration!**
